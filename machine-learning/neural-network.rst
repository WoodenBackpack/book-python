**************
Neural Network
**************

What Is a Neural Network?
=========================
It’s a technique for building a computer program that learns from data. It is based very loosely on how we think the human brain works. First, a collection of software "neurons" are created and connected together, allowing them to send messages to each other. Next, the network is asked to solve a problem, which it attempts to do over and over, each time strengthening the connections that lead to success and diminishing those that lead to failure. For a more detailed introduction to neural networks, `Michael Nielsen’s Neural Networks <http://neuralnetworksanddeeplearning.com/index.html>`_ and `Deep Learning <http://www.deeplearningbook.org/>`_ is a good place to start. For a more technical overview, try Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

.. figure:: img/deep-neural-network.png
    :scale: 75%
    :align: center

    Deep Neural Network.

Tools
=====
* TensorFlow (Google) - http://playground.tensorflow.org/

Inception
---------
* One of Google's best image classifiers
* Open Source
* Trained on 1.2 milion images
* Training took 2 weeks on 8GPU machine

Działanie na sieciach neuronowych
=================================

Construction
------------

Learning
--------

Optimizing
----------

Retraining
----------
* Also known as Transfer Learning
* Saves a lot of time
* Uses prior work

Przetwarzanie obrazów
=====================

Flattening image
----------------
.. figure:: img/features-images.png
    :scale: 75%
    :align: center

    In Image processing files and image pixels are features.

* Używanie "raw pixels" as features
* Classifier does the rest
* Flatten image: 2D array -> 1D by unstacking rows and lining them up (reshape array):

    .. code-block:: python

        import matplotlib.pyplot as plt

        def display(i):
            img = test_data[i]
            plt.title('Example %d. Label: %d' % (i, test_labels[i]))
            plt.imshow(img.reshape((28,28)), cmap=plt.cm.gray_r)

.. figure:: img/deep-neural-networks-mnist-segmented.png
    :scale: 75%
    :align: center

    Segmented Digit

Weight adjusted by gradient descent
-----------------------------------
* Begin with random weight
* Gradually adjust to better values
* Evaluate accuracy

.. figure:: img/deep-neural-networks-mnist-pixels.png
    :scale: 50%
    :align: center

    Compare middle image pixel.

Visualize weights
-----------------
.. figure:: img/deep-neural-networks-mnist-weights.png
    :scale: 75%
    :align: center

    Visualize the the weights in the TensorFlow Basic MNIST

Przykłady praktyczne
====================

Image Classification using ``TensorFlow for Poets``
---------------------------------------------------
* https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#1

.. code-block:: console

    # download around 218MB of data
    $ curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
    $ tar xzf flower_photos.tgz
    $ ls flower_photos

.. warning:: Training on this much data can take 30+ minutes on a small computer. If you want to reduce data:

    .. code-block:: console

        $ ls flower_photos/roses | wc -l
        $ rm flower_photos/*/[3-9]*
        $ ls flower_photos/roses | wc -l

.. code-block:: python

    from sklearn import metrics
    from sklearn import model_selection
    import tensorflow as tf
    from tensorflow.contrib import learn


    # Load dataset
    iris = learn.datasets.load_dataset('iris')
    x_train, x_test, y_train, y_test = model_selection.train_test_split(
        iris.data,
        iris.target,
        test_size=0.2,
        random_state=42
    )

    # Build 3 layer Deep Neural Network (DNN) with 10, 20, 10 units respectively.
    classifier = learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)

    # Fit and predict.
    classifier.fit(x_train, y_train, steps=200)
    score = metrics.accuracy_score(y_test, classifier.predict(x_test))

    print(f'Accuracy {score:f}')

.. code-block:: console

    $ curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/r1.1/tensorflow/examples/image_retraining/retrain.py

    $ python retrain.py \
      --bottleneck_dir=bottlenecks \
      --how_many_training_steps=500 \
      --model_dir=inception \
      --summaries_dir=training_summaries/basic \
      --output_graph=retrained_graph.pb \
      --output_labels=retrained_labels.txt \
      --image_dir=flower_photos

    [...]
    2017-07-01 11:10:43.635017: Step 499: Train accuracy = 88.0%
    2017-07-01 11:10:43.635265: Step 499: Cross entropy = 0.455413
    2017-07-01 11:10:44.201455: Step 499: Validation accuracy = 92.0% (N=100)

    Final test accuracy = 87.3% (N=331)

    $ curl -L https://goo.gl/3lTKZs > label_image.py

    $ python label_image.py flower_photos/daisy/21652746_cc379e0eea_m.jpg
    daisy (score = 0.98659)
    sunflowers (score = 0.01068)
    dandelion (score = 0.00204)
    tulips (score = 0.00063)
    roses (score = 0.00007)

    $ python label_image.py flower_photos/roses/2414954629_3708a1a04d.jpg
    roses (score = 0.84563)
    tulips (score = 0.13727)
    dandelion (score = 0.00897)
    sunflowers (score = 0.00644)
    daisy (score = 0.00169)


Handwritten digits recognition (MNIST) with ``tf.contrib.learn``
----------------------------------------------------------------

.. figure:: img/deep-neural-networks-mnist-overview.png
    :scale: 50%
    :align: center

    Handwritten digits recognition also known as MNIST is equivalent to "hello world" in visual Machine Learning world.

.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline
    import tensorflow as tf

    learn = tf.contrib.learn
    tf.logging.set_verbosity(tf.logging.ERROR)

    # Import the dataset
    mnist = learn.datasets.load_dataset('mnist')
    data = mnist.train.images
    labels = np.asarray(mnist.train.labels, dtype=np.int32)
    test_data = mnist.test.images
    test_labels = np.asarray(mnist.test.labels, dtype=np.int32)

    # There are 55k examples in train, and 10k in eval. You may wish to limit the size to experiment faster.
    max_examples = 10000
    data = data[:max_examples]
    labels = labels[:max_examples]

    def display(i):
        img = test_data[i]
        plt.title('Example %d. Label: %d' % (i, test_labels[i]))
        plt.imshow(img.reshape((28,28)), cmap=plt.cm.gray_r)


    # You can display output:
    # display(0)
    # display(1)
    # display(8)
    # print len(data[0])


    # Fit a Linear Classifier
    feature_columns = learn.infer_real_valued_columns_from_input(data)

    # n_classes = 10 because we have 10 digits
    classifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)
    classifier.fit(data, labels, batch_size=100, steps=1000)

    # Evaluate accuracy
    classifier.evaluate(test_data, test_labels)
    print(classifier.evaluate(test_data, test_labels)["accuracy"])
    # output: 0.9141


    # Classify a few examples

    # here's one it gets right
    print ("Predicted %d, Label: %d" % (classifier.predict(test_data[0]), test_labels[0]))
    display(0)

    # and one it gets wrong
    print ("Predicted %d, Label: %d" % (classifier.predict(test_data[8]), test_labels[8]))
    display(8)

    # Visualize learned weights
    weights = classifier.weights_
    f, axes = plt.subplots(2, 5, figsize=(10,4))
    axes = axes.reshape(-1)
    for i in range(len(axes)):
        a = axes[i]
        a.imshow(weights.T[i].reshape(28, 28), cmap=plt.cm.seismic)
        a.set_title(i)
        a.set_xticks(()) # ticks be gone
        a.set_yticks(())
    plt.show()



Zadania praktyczne
==================



Kto jest na zdjęciu?
--------------------
Stwórz zbiór obrazów zawierający tylko twarze osób:

    - twoje,
    - twojego przyjaciela/przyjacółki.

Postaraj się aby zdjęcia były na wprost. Naucz algorytm ich rozpoznawania i przedstaw Mu jakąś nową twarz (twoją lub przyjaciela i zobacz czy potrafi rozpoznać i z jaką dokładnością.
