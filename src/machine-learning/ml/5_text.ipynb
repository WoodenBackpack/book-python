{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://sigdelta.com/assets/images/sages-sd-logo.png)\n",
    "\n",
    "# Analiza danych i uczenie maszynowe w Python\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "## Dane tekstowe\n",
    "\n",
    "W tym notebooku prezentujemy podejście do analizy, wektoryzacji danych i uczenia maszynowego z użyciem scikit-learn i innych pakietów w Python. Kroki te są wstępem do uczenia maszynowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Parametry wykresów\n",
    "mpl.style.use('ggplot')\n",
    "mpl.rcParams['figure.figsize'] = (8,6)\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macierz Term-Document\n",
    "\n",
    "[Macierz Term-Document (TD)](https://en.wikipedia.org/wiki/Document-term_matrix), jest to macierz, w której kolumny stanowią unikalne słowa, a wiersze stanowią wartości ile razy dane słowo wystąpiło w dokumencie. Dla przykładu (za Wikipedią), jeżeli mamy dwa dokumenty:\n",
    "\n",
    "* D1 = \"I like databases\"\n",
    "* D2 = \"I hate databases\"\n",
    "\n",
    "to macierz TD możemy przedstawić jako\n",
    "\n",
    "|    | I | like | hate | databases |\n",
    "|----|---|------|------|-----------|\n",
    "| **D1** | 1 | 1    | 0    | 1         |\n",
    "| **D2** | 1 | 0    | 1    | 1         |\n",
    "\n",
    "Poniżej ten sam przykład zrealizowany w Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>I like databases Databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>I hate databases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs                       lines\n",
       "0   D1  I like databases Databases\n",
       "1   D2            I hate databases"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.DataFrame({\n",
    "    'docs': ['D1', 'D2'],\n",
    "    'lines': ['I like databases Databases', 'I hate databases']\n",
    "})\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>I like databases Databases</td>\n",
       "      <td>[i, like, databases, databases]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>I hate databases</td>\n",
       "      <td>[i, hate, databases]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs                       lines                            words\n",
       "0   D1  I like databases Databases  [i, like, databases, databases]\n",
       "1   D2            I hate databases             [i, hate, databases]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['words'] = sample.lines.str.strip().str.lower().str.split('[\\W_]+')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D2</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D2</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D2</td>\n",
       "      <td>databases</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs       word\n",
       "0   D1          i\n",
       "1   D1       like\n",
       "2   D1  databases\n",
       "3   D1  databases\n",
       "4   D2          i\n",
       "5   D2       hate\n",
       "6   D2  databases"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list()\n",
    "for row in sample[['docs', 'words']].iterrows():\n",
    "    r = row[1]\n",
    "    for word in r.words:\n",
    "        rows.append((r.docs, word))\n",
    "\n",
    "words = pd.DataFrame(rows, columns=['docs', 'word'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "word  databases  hate    i  like\n",
       "docs                            \n",
       "D1          2.0   0.0  1.0   1.0\n",
       "D2          1.0   1.0  1.0   0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pivot_table(index='docs', \n",
    "                  columns='word', \n",
    "                  aggfunc=lambda v: v['word'].count())\\\n",
    "    .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Używając Pandas obliczyć macierz TD dla książek.\n",
    "1. Co może wpłynąć na wynik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I like databases Databases\n",
       "1              I hate databases\n",
       "Name: lines, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = count_vectorizer.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1],\n",
       "       [1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['databases', 'hate', 'like']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      databases  hate  like\n",
       "docs                       \n",
       "D1            2     0     1\n",
       "D2            1     1     0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             columns=count_vectorizer.get_feature_names(), \n",
    "             index=sample['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Kolejną techniką jest poznane uprzednio [Term Frequency–Inverse Document Frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Jest on popularnym algorytmem do analizy danych tekstowych, używany dość często w pozyskiwaniu danych (data mining).\n",
    "\n",
    "Poniżej przykładowe obliczenie TF-IDF z użyciem wektoryzera scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_vectorizer.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81818021, 0.        , 0.57496187],\n",
       "       [0.57973867, 0.81480247, 0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['databases', 'hate', 'like']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      databases      hate      like\n",
       "docs                               \n",
       "D1     0.818180  0.000000  0.574962\n",
       "D2     0.579739  0.814802  0.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "            columns=tfidf_vectorizer.get_feature_names(), \n",
    "            index=sample['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miary podobieństwa\n",
    "\n",
    "Jest wiele miar podobieństwa, które liczą jak odległe są pary wartości od siebie. Miary mogą być używane zarówno do wektorów oraz macierzy, jak i słów oraz dokumentów; zobacz [ciekawy opis](http://dataconomy.com/2015/04/implementing-the-five-most-popular-similarity-measures-in-python/) miar wraz z implementacją w Pythonie. \n",
    "\n",
    "Pierwszą miarą, która jest standardową miarą podobieństwa dwóch stringów jest [miara Levenshteina](https://en.wikipedia.org/wiki/Levenshtein_distance). Jest wiele implementacji używania tej miary, niemniej, najłatwiej dostępną implementacją jest zawarta w [difflib](https://docs.python.org/3/library/difflib.html), która jest częścią standardowej biblioteki Pythona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib \n",
    "\n",
    "def similarity(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "similarity('cat', 'cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>lines</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>I like databases Databases</td>\n",
       "      <td>[i, like, databases, databases]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>I hate databases</td>\n",
       "      <td>[i, hate, databases]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docs                       lines                            words\n",
       "0   D1  I like databases Databases  [i, like, databases, databases]\n",
       "1   D2            I hate databases             [i, hate, databases]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6190476190476191"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(sample.at[0, 'lines'], sample.at[1, 'lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity('cat', 'catepillar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku dokumentów tekstowych, znane są przynajmniej dwie popularne miary:\n",
    "\n",
    "* cosinusowa\n",
    "* Jaccarda\n",
    "\n",
    "**Miaria cosinusowa** zdefiniowana jest jako kąt między dwoma wektorami:\n",
    "\n",
    "$$\n",
    "sim(A, B) = \\cos(\\Theta) = \\frac{A \\cdot B}{\\Vert A \\Vert \\cdot \\Vert B \\Vert}\n",
    "$$\n",
    "\n",
    "Zatem miara wymaga formy zwektowyzowanej do obliczenia wartości; musimy się najpierw posłużyć którymś wektoryzatorem aby otrzymać macierz a potem policzyć miarę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.63245553],\n",
       "       [0.63245553, 1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Miara Jaccarda** operuje z kolei na zbiorach i zdefiniowana jest jako:\n",
    "\n",
    "$$\n",
    "sim(A, B) = \\frac{|A \\cap B|}{| A \\cup B |}\n",
    "$$\n",
    "\n",
    "Zatem dla naszego przykładu wygląda to następująco:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i', 'like', 'databases', 'databases'], ['i', 'hate', 'databases'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sample.words[0]\n",
    "B = sample.words[1]\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', 'databases'} {'like', 'databases', 'i', 'hate'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim_jaccard(A, B):\n",
    "    a = set(A)\n",
    "    b = set(B)\n",
    "    i = set.intersection(a, b)\n",
    "    u = set.union(a, b)\n",
    "    print(i, u)\n",
    "    return len(i)/len(u)\n",
    "\n",
    "sim_jaccard(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "list(set(A))\n",
    "jaccard_similarity_score(list(set(A)), list(set(B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transformatory i pipeliny\n",
    "\n",
    "Scikit-learn wprowadził szereg ułatwień do przetwarzania danych i tworzenia modeli. W ogólności, możemy korzystać z 3 podstawowych elementów:\n",
    "\n",
    "* [transformer](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)\n",
    "* [estimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)\n",
    "* [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n",
    "\n",
    "Transformetry (funkcje zmieniające dane) i estymatory (modele do wyuczenia) łączy się w pipeliny; więcej o tym można przeczytać [w dokumentacji](http://scikit-learn.org/stable/modules/pipeline.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = CountVectorizer().fit_transform(sample['lines'])\n",
    "tfidf_transformer.fit_transform(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...'tfidftransformer', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "tfidf_pipeline = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81818021, 0.        , 0.57496187],\n",
       "       [0.57973867, 0.81480247, 0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_pipeline.fit_transform(sample['lines'])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databases</th>\n",
       "      <th>hate</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.579739</td>\n",
       "      <td>0.814802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      databases      hate      like\n",
       "docs                               \n",
       "D1     0.818180  0.000000  0.574962\n",
       "D2     0.579739  0.814802  0.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=tfidf_pipeline.steps[0][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można też samemu nazywać elementy pipelinu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-c74d8b759810>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'steps' is not defined"
     ]
    }
   ],
   "source": [
    "list(steps.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = {\n",
    "    'count_vect': CountVectorizer(),\n",
    "    'tfidf_trans': TfidfTransformer()\n",
    "}\n",
    "\n",
    "# Pipeline oczekuje kroków jako listy z krotkami (nazwa, obiekt)\n",
    "tfidf_pipeline = Pipeline(list(steps.items()))\n",
    "tfidf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co tu się dzieje?\n",
    "steps['count_vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Wykorzystaj narzędzia scikit-learn do stworzenie pełnego pipelinu wykonującego wektoryzację z TF-IDF dla danych z książek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje użytkownika\n",
    "\n",
    "Niekiedy mamy potrzebę korzystania z funkcji wybiegających poza zestaw dostępny w scikit-learn. Są dwie metody:\n",
    "\n",
    "* [FunctionTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)\n",
    "* Klasa dziedzicząca po [BaseEstimator i TransformerMixin](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py)\n",
    "\n",
    "Obecnie najpierw zalecany jest FunctionTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "@np.vectorize\n",
    "def replace_database(linia):\n",
    "    return re.sub('database', 'DB', linia, flags=re.IGNORECASE)\n",
    "\n",
    "replace_database(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "replace_func = FunctionTransformer(replace_database, validate=False)\n",
    "replace_func.fit_transform(sample['lines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = make_pipeline(replace_func, TfidfVectorizer())\n",
    "new_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_pipeline.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=new_pipeline.steps[1][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Drugą metodą tworzenia funkcji użytkownika jest stworzenie klasy, która dziedziczy po klasach `BaseEstimator` i `TransformerMixin`, czyli de facto jak to jest robione dla innych transformerów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ReplaceDatabaseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_from='database', replace_to='DB'):\n",
    "        self.replace_from = replace_from\n",
    "        self.replace_to = replace_to\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def _replace_str(self, line):\n",
    "        return re.sub(self.replace_from, \n",
    "                      self.replace_to, \n",
    "                      line, \n",
    "                      flags=re.IGNORECASE)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        func = np.vectorize(lambda line: self._replace_str(line))\n",
    "        return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline2 = make_pipeline(ReplaceDatabaseTransformer(), TfidfVectorizer())\n",
    "new_pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_pipeline2.fit_transform(sample['lines'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             index=sample['docs'], \n",
    "             columns=new_pipeline2.steps[1][1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Napisz funkcję zamieniającą wszystkie napotkane [URLe](https://pl.wikipedia.org/wiki/Uniform_Resource_Locator) na stałą wartość, która może być podawana jako argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja danych tekstowych\n",
    "\n",
    "W tej części notebooka przeprowadzimy klasyfikację danych tekstowych. Do analiz będziemy wykorzystywać kolekcję wiadomości SMS, zawierających spam i prawdziwe wiadomości, dostępnym w [repozytorium UCI](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Poniższy kod pobiera dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_path = 'data'\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "file_name = url.split('/')[-1]\n",
    "dest_file = os.path.join(data_path, file_name) \n",
    "\n",
    "data_file = 'SMSSpamCollection'\n",
    "data_full = os.path.join(data_path, data_file)\n",
    "\n",
    "urllib.request.urlretrieve(url, dest_file)\n",
    "\n",
    "with zipfile.ZipFile(dest_file) as zip_file:\n",
    "    zip_file.extract(data_file, path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  is_spam                                               text\n",
       "0     ham  Go until jurong point, crazy.. Available only ...\n",
       "1     ham                      Ok lar... Joking wif u oni...\n",
       "2    spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3     ham  U dun say so early hor... U c already then say...\n",
       "4     ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv(data_full, \n",
    "                  sep='\\t', \n",
    "                  names=['is_spam', 'text'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "       is_spam                    text\n",
      "count     4457                    4457\n",
      "unique       2                    4189\n",
      "top        ham  Sorry, I'll call later\n",
      "freq      3860                      24\n",
      "\n",
      "Test set:\n",
      "       is_spam                    text\n",
      "count     1115                    1115\n",
      "unique       2                    1092\n",
      "top        ham  Sorry, I'll call later\n",
      "freq       965                       6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sms, test_sms = train_test_split(sms, test_size=0.2)\n",
    "\n",
    "print('Train set:')\n",
    "print(train_sms.describe())\n",
    "print()\n",
    "print('Test set:')\n",
    "print(test_sms.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naszym zadaniem jest wytrenowanie klasyfikatora, który klasyfikuje wiadomość SMS jako spam. Podobne zadanie z dość szerokim opisem można znaleźć w [tym wpisie](https://radimrehurek.com/data_science_python/). \n",
    "\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Zaczniemy od klasyfikatora [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html). Używa on [twierdzenia Bayesa](https://en.wikipedia.org/wiki/Bayes%27_theorem) do obliczenia prawdopodobieństw poszczególnych klas w zależności od dostępnych opcji, przy założeniu niezależności zmiennych losowych (stąd naiwny). Rozpatrzmy przykład ([źródło](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)):\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41-850x310.png)\n",
    "\n",
    "Poniżej przykład obliczenia prawdopodobieństwa na podstawie metody Bayesa.\n",
    "\n",
    "$$P(Yes | Sunny) = \\frac{P( Sunny | Yes)  P(Yes)}{P (Sunny)}$$\n",
    "$$P (Sunny |Yes) = 3/9 = 0.33$$\n",
    "$$P(Sunny) = 5/14 = 0.36$$ \n",
    "$$P( Yes)= 9/14 = 0.64$$\n",
    "$$P (Yes | Sunny) = \\frac{0.33 \\cdot 0.64}{0.36} = 0.60$$\n",
    "\n",
    "Poniżej, przykład modelu detekcji spamu z użuciem metody Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x7743 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 59338 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(train_sms['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_detector = MultinomialNB().fit(X, train_sms['is_spam'])\n",
    "spam_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spam', 'spam', 'FreeMsg>FAV XMAS TONES!Reply REAL')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 17\n",
    "train_sms.iloc[i, 0], spam_detector.predict(X[i])[0], train_sms.iloc[i, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[965   0]\n",
      " [ 43 107]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98       965\n",
      "       spam       1.00      0.71      0.83       150\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X = vectorizer.transform(test_sms['text'])\n",
    "\n",
    "y_pred = spam_detector.predict(X)\n",
    "y_true = test_sms['is_spam']\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[949   0]\n",
      " [ 66 100]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      1.00      0.97       949\n",
      "       spam       1.00      0.60      0.75       166\n",
      "\n",
      "avg / total       0.94      0.94      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sms = pd.read_csv(data_full, sep='\\t', names=['is_spam', 'text'])\n",
    "train_sms, test_sms = train_test_split(sms, test_size=0.2)\n",
    "\n",
    "steps = [('tfidf', TfidfVectorizer()), ('cls', MultinomialNB())]\n",
    "nb_pipe = Pipeline(steps=steps)\n",
    "nb_pipe.fit(train_sms['text'], train_sms['is_spam'])\n",
    "\n",
    "y_pred = nb_pipe.predict(test_sms['text'])\n",
    "y_true = test_sms['is_spam']\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zbuduj pipeline wektoryzatora i modelu.\n",
    "1. Użyj `GridSearchCV` do znalezienia najlepszego modelu.\n",
    "1. Popraw tokenizację w celu poprawienia jakości klasyfikacji.\n",
    "1. Użyj innych klasyfikatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelowanie tematów\n",
    "\n",
    "Modelowanie tematów (topic modelling) jest to zadanie ekstrakcji ważnych elementów z tekstu, które są jego tematami. Wiodącą biblioteką w pythonie do tego celu jest [gensim](https://radimrehurek.com/gensim/). \n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Jest wiele ciekawych algorytmów wykonujących to zadanie, jednak jednym z najgłośniejszych algorytmów do ekstrakcji tematów jest [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*II7wZlKViCt4ssBm.png)\n",
    "\n",
    "Jedną z najskuteczniejszych implementaci LDA w Pythonie jest implementacja dostępna w gensim, która posiada też dość rozbudowaną [dokumentację](https://radimrehurek.com/gensim/models/ldamodel.html). Wykorzystajmy zatem LDA do naszego zadania.\n",
    "\n",
    "Do analizy wykorzystamy zbiór [20 newsgroups z scikit-learn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                                remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, ...,  9,  4,  9])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawęzimy trochę próbkę, żeby uczenie było szybsze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "newsgroups_samples = newsgroups.data[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw tworzymy korpus dla LDA używając narzędzi gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "words = [nltk.regexp_tokenize(d.lower(), '\\w+') for d in newsgroups_samples]\n",
    "\n",
    "dictionary = corpora.Dictionary(words)\n",
    "corpus = [dictionary.doc2bow(text) for text in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Słownik posiada ID słowa jako klucz i słowo jako wartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: well\n",
      "1: i\n",
      "2: m\n",
      "3: not\n",
      "4: sure\n",
      "5: about\n",
      "6: the\n",
      "7: story\n",
      "8: nad\n",
      "9: it\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in islice(dictionary.items(), 10):\n",
    "    print('{}: {}'.format(k, v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korpus posiada listę krotek, lista na dokument; krotki zawierają pary ID słowa i jego liczność w danych dokumencie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 4), (2, 1), (3, 3), (4, 1), (5, 1), (6, 17), (7, 1), (8, 1), (9, 2), (10, 1), (11, 2), (12, 1), (13, 2), (14, 1), (15, 1), (16, 8), (17, 1), (18, 1), (19, 5), (20, 4), (21, 4), (22, 4), (23, 1), (24, 4), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 4), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 3), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 2), (54, 1), (55, 2), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1)], [(0, 1), (1, 2), (2, 1), (5, 1), (6, 2), (9, 1), (17, 2), (19, 2), (21, 1), (23, 1), (24, 2), (44, 1), (46, 2), (53, 2), (60, 1), (63, 1), (75, 3), (79, 1), (91, 1), (92, 1), (103, 1), (104, 1), (105, 7), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 2), (114, 1), (115, 1), (116, 2), (117, 1), (118, 1), (119, 2), (120, 1), (121, 1), (122, 1), (123, 1), (124, 3), (125, 2), (126, 1), (127, 3), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 2), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Dla 10 pierwszych dokumentów wypisz słowa i ich liczność."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wytrenujmy sam model LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x1a04ff57630>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, alpha=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 0.99340100552816657)]\n",
      "[(7, 0.98946292387574297)]\n",
      "[(1, 0.46998139553451107), (7, 0.52251416682123741)]\n",
      "[(4, 0.16633537596457032), (7, 0.82828027035858187)]\n",
      "[(0, 0.6996453734915814), (7, 0.28425805069277971)]\n",
      "[(9, 0.96061919565863985)]\n",
      "[(0, 0.99818563354973222)]\n",
      "[(0, 0.84510135298355327), (4, 0.14878333739966915)]\n",
      "[(0, 0.22123840595863969), (9, 0.76934711243078313)]\n",
      "[(9, 0.99313520855566895)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(model.get_document_topics(corpus[i], minimum_probability=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.049394249428051845),\n",
       " (75, 0.019961484867415665),\n",
       " (24, 0.018927750832629032),\n",
       " (1, 0.018355421479303465),\n",
       " (46, 0.016457198658624449),\n",
       " (32, 0.015065313667161167),\n",
       " (53, 0.012715351470155322),\n",
       " (21, 0.0094795785434583398),\n",
       " (16, 0.0087506787626141138),\n",
       " (105, 0.0082980143790526185)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "model.get_topic_terms(topicid=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"the\" + 0.020*\"of\" + 0.019*\"to\" + 0.018*\"i\" + 0.016*\"a\" + 0.015*\"in\" + 0.013*\"and\" + 0.009*\"s\" + 0.009*\"is\" + 0.008*\"you\"'),\n",
       " (1,\n",
       "  '0.038*\"the\" + 0.024*\"to\" + 0.020*\"of\" + 0.018*\"is\" + 0.017*\"a\" + 0.016*\"and\" + 0.016*\"in\" + 0.015*\"that\" + 0.011*\"i\" + 0.010*\"you\"'),\n",
       " (2,\n",
       "  '0.019*\"and\" + 0.018*\"to\" + 0.016*\"a\" + 0.015*\"the\" + 0.014*\"i\" + 0.013*\"for\" + 0.010*\"in\" + 0.010*\"it\" + 0.009*\"is\" + 0.009*\"that\"'),\n",
       " (3,\n",
       "  '0.038*\"the\" + 0.017*\"a\" + 0.017*\"to\" + 0.016*\"0\" + 0.016*\"and\" + 0.016*\"of\" + 0.012*\"i\" + 0.012*\"1\" + 0.012*\"that\" + 0.012*\"it\"'),\n",
       " (4,\n",
       "  '0.041*\"the\" + 0.026*\"to\" + 0.023*\"i\" + 0.022*\"and\" + 0.019*\"a\" + 0.017*\"of\" + 0.014*\"that\" + 0.012*\"is\" + 0.011*\"in\" + 0.010*\"you\"'),\n",
       " (5,\n",
       "  '0.040*\"the\" + 0.030*\"to\" + 0.024*\"and\" + 0.019*\"a\" + 0.014*\"is\" + 0.013*\"of\" + 0.013*\"that\" + 0.011*\"in\" + 0.010*\"i\" + 0.009*\"for\"'),\n",
       " (6,\n",
       "  '0.051*\"the\" + 0.022*\"i\" + 0.022*\"and\" + 0.016*\"of\" + 0.014*\"a\" + 0.012*\"to\" + 0.012*\"in\" + 0.010*\"for\" + 0.009*\"is\" + 0.008*\"it\"'),\n",
       " (7,\n",
       "  '0.053*\"the\" + 0.028*\"to\" + 0.025*\"of\" + 0.023*\"a\" + 0.019*\"and\" + 0.016*\"in\" + 0.012*\"is\" + 0.012*\"that\" + 0.011*\"i\" + 0.010*\"it\"'),\n",
       " (8,\n",
       "  '0.037*\"the\" + 0.026*\"of\" + 0.018*\"to\" + 0.013*\"that\" + 0.013*\"and\" + 0.012*\"i\" + 0.011*\"a\" + 0.011*\"in\" + 0.008*\"on\" + 0.008*\"s\"'),\n",
       " (9,\n",
       "  '0.047*\"the\" + 0.022*\"of\" + 0.022*\"a\" + 0.019*\"to\" + 0.018*\"it\" + 0.017*\"i\" + 0.015*\"and\" + 0.014*\"in\" + 0.014*\"is\" + 0.013*\"that\"')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zmień ilość tematów; czy coś się zmieniło?\n",
    "1. Zobacz jak tematy mapują się na klasy.\n",
    "1. Popraw tokenizację; możesz np.:\n",
    "    - usunąć nie-słowa\n",
    "    - usunąć wyrazy ze stop listy\n",
    "    - sprowadzić słowa do wspólnej wielkości znaku\n",
    "1. Sprawdź inne metody budowania korpusu, np.:\n",
    "    - macierz TD\n",
    "    - TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "from gensim.sklearn_api.text2bow import Text2BowTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = {\n",
    "    'text2bow': Text2BowTransformer(),\n",
    "    'lda': LdaTransformer(num_topics=10)\n",
    "}\n",
    "\n",
    "p = Pipeline(list(steps.items()))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.fit_transform(newsgroups_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps['lda'].gensim_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps['text2bow'].gensim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.transform(['ala ma kota', 'lala'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Możemy też zastosować wbudowaną metodę scikit-learn; zobacz [dokumentację](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funkcja użytkowa do wyświetlania tematów.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=1000,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(newsgroups_samples)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zrób pipeline z powyższego przetwarzania.\n",
    "1. Spróbuj użyć TF-IDF.\n",
    "1. Zmień liczbę tematów `n_components` i zobacz co się zmieni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization\n",
    "\n",
    "Kolejnym algorytmem często używanym do ekstrakcji tematów jest [Non-negative Matrix Factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization). \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)\n",
    "\n",
    "Metoda ta używa faktoryzacji do przybliżenia macierzy V jako iloczynu macierzy H i W. W wyniku działania algorytmu macierze H i W mają właściwości klasteryzacyjne danych w macierzy V. Dokładnie W staje się macierzą centroidów a H indykatorem przyporządkowania do klastrów poszczególnych elementów macierzy V.\n",
    "\n",
    "W praktyce często stosuje się tą metodę jako zamiennik PCA, lecz tylko dla danych pozytywnych, oraz do ekstrakcji tematów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(newsgroups_samples)\n",
    "\n",
    "nmf = NMF(n_components=10, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zmień ilość tematów `n_components`.\n",
    "1. Spróbuj innej funkcji straty `beta_loss`; zobacz [dokumentację](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
